\documentclass[12pt]{article}

% This first part of the file is called the PREAMBLE. It includes
% customizations and command definitions. The preamble is everything
% between \documentclass and \begin{document}.

\usepackage[margin=1in]{geometry}  % set the margins to 1in on all sides
\usepackage{graphicx}              % to include figures
\usepackage{amsmath,bm}            % great math stuff
\usepackage{amsfonts}              % for blackboard bold, etc
\usepackage{amsthm}                % better theorem environments
\usepackage{listings}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage[]{algorithm2e}
\usepackage{parskip} 			   % no paragraph indentation

\usetikzlibrary{arrows,automata}


% various theorems, numbered by section

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{dfn}[thm]{Definition}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}

\DeclareMathOperator{\id}{id}

\newcommand{\bd}[1]{\mathbf{#1}}  % for bolding symbols
\newcommand{\RR}{\mathbb{R}}      % for Real numbers
\newcommand{\ZZ}{\mathbb{Z}}      % for Integers
\newcommand{\col}[1]{\left[\begin{matrix} #1 \end{matrix} \right]}
\newcommand{\comb}[2]{\binom{#1^2 + #2^2}{#1+#2}}

\lstset{ % General setup for the package
    language={[LaTeX]TeX},
    basicstyle=\footnotesize\sffamily,
    tabsize=4,
    columns=fixed,
    keepspaces,
    commentstyle=\color{red},
    keywordstyle=\color{blue},
    xleftmargin=.1\textwidth,
    xrightmargin=.1\textwidth
}

\begin{document}

\nocite{*}


\title{Randomized Algorithms \\
       Assignment 3}

\author{Viktor Hansen \& Simon Rueskov Schleicher}

\maketitle

\begin{abstract}
  This is the third weekly assignment for the Randomized Algorithms course offered at The Department of Computer Science, Uni. Copenhagen.
\end{abstract}

\pagebreak

\section*{Exercise 3.10}
We wish to find an upper bound on $\mathbf{Pr} \left[ X > \beta n \ln n \right] \leq \mathbf{Pr} \left[ \left| X - \mu_X \right| \geq \beta n \ln n - \mu_X \right]$. Now, setting $t\sigma_X = \beta n \ln n - \mu_X$, we get $t = \frac{\beta n \ln n - \mu_X}{\sigma_X}$. Inserting and applying Chebyshev's inequality yields
\begin{align*}
\mathbf{Pr} \left[ \left| X - \mu_X \right| \geq \frac{\beta n \ln n - \mu_X}{\sigma_X} \sigma_X \right] \leq \frac{\sigma_X^2}{(\beta n \ln n - \mu_X)^2}
\end{align*}
Inserting $\mu_X = \mathbf{E} \left[ X \right] = n \ln n + O(n)$ gives
\begin{align*}
\frac{\sigma_X^2}{(\beta n \ln (n) - \mu_X)^2} &= \frac{\sigma_X^2}{(\beta n \ln (n) - n \ln(n) - O(n))^2} = \frac{\sigma_X^2}{(n \ln(n)(\beta - 1) - O(n))^2} \\
&= \frac{\sigma_X^2}{n^2 \ln^2(n) (\beta - 1)^2 + O(n^2) - O(n^2 \ln(n))} \\
&\leq \frac{\sigma_X^2}{O(n^2 \ln^2(n)) - O(n^2 \ln(n))} \\
&= \frac{\sigma_X^2}{O(n^2 \ln^2(n)} \\
&\leq \lim_{n \rightarrow \infty} \left( O \left( \frac{\sigma_X^2}{n^2} \right) \right) \frac{1}{O(\ln^2(n)} \\
& = O \left( \frac{1}{\ln^2(n)} \right)
\end{align*}
The last two steps follow from $\sigma_X^2/n^2$ being monotonically increasing and converging for large enough $n$.

\section*{Problem 3}
Consider a treap $T=(e_1, e_2, ..., e_n) = ((v_1,p_1), (v_2,p_2), ..., (v_n,p_n))$ with an element $e_k = (v_k, -\infty) \in T$ of rank $k$. We wish to analyze the expected depth of $e_j$ in $T$. Let $X_i = 1$ if $e_i$ is an ancestor of $e_k$ and $X_i=0$ otherwise. Then the depth of $e_k$ is given by $X=1 + \sum_{i=1}^{n}X_i$ and
\begin{align*}
\mathbf{E}\left[ X \right] = 1 + \mathbf{E}\left[ \sum\limits_{i=1}^{n}X_i \right] = 1 + \sum\limits_{i=1}^{n}\mathbf{E}\left[X_i \right] 
\end{align*}
Since $e_k$ has rank $k$, $e_i$ is an ancestor of $e_k$ exactly when $e_i$ has the highest priority of the elements $e_i, ..., e_k$, when $i < k$ and $e_k, ..., e_i$ otherwise. Since the priorities are chosen at random, the probabilities of these events are given by $1/(k-i)$ and $1/(i-k)$, respectively. We then have $\mathbf{Pr}\left[ X_i = 1 \right] = 1 / |i-k|$ and
\begin{align*}
1 + \sum\limits_{i=1}^{n}\mathbf{E}\left[X_i \right] &= 1 + \sum\limits_{i=1}^{n}\mathbf{Pr}\left[X_i = 1 \right] \\
&= 1 + \sum\limits_{i=1}^{n} 1 / |i-k| \\
&= 1 +\sum\limits_{i=1}^{k-1} \frac{1}{k-i} + \sum\limits_{i=k+1}^{n} \frac{1}{i-k} \\
&= 1 +H_{k-1} + H_{n-k}
\end{align*}
Subtracting the expected depth of $e_k$ from the expected depth of an element with rank $k$ and a random priority (lemma 8.6) yields
\begin{align*}
1 + H_{k-1} + H_{n-k} - H_k - H_{n-k+1} + 1 &= 1 + (H_{k-1} - H_k) + (H_{n-k} - H_{n-k+1}) + 1 \\
&= 1 -\frac{1}{k} +1 - \frac{1}{n-k+1} \\
&= \mathbf{E}\left[ R_x \right] + \mathbf{E}\left[ L_x \right]
\end{align*}
which exactly is the expected number of rotations required when deleting an element of rank $k$. This makes sense, because when deleting the element, we change its priority to $\infty$ and make rotations until it reaches the bottom. Each rotation moves it down 1 level. So the depth of the bottom (when the priority is negative infinity) should be the depth when we started plus the number of rotations.


\section*{Problem 4}
Consider the 2-level hashing scheme with $h_a$ used for the hash-function. Pick random, odd $a \in [2^w]$, and let $u = 2^w$ and $t = 2^\ell$ where $\ell \leq w$. Let $C_{x,y} = h_a(x) == h_a(y)$, i.e. if there is a collision between $x$ and $y$ and $C = \sum_{x \neq y} C_{x,y}$ denote the total number of collisions. Then $\mathbf{E} \left[ C \right] = \mathbf{E}\left[ \sum_{x \neq y} C_{x,y} \right] \leq \sum_{x \neq y} 2/2^{\ell} = \binom{n}{2}2/2^\ell = n(n-1)/2^\ell$. For a set of size $n$, we choose $\ell = \lfloor \log_2(n-1) \rfloor + 2$, for which $\mathbf{E}[C] = n(n-1)/2^{\lfloor \log_2(n-1) \rfloor + 2} \leq n(n-1)/2^{\log_2(n-1) + 1} = n/2$. Pick $h_a$ s.t. $C \leq n$. Then from applying Markov's inequality we get that $\mathbf{Pr}[X \geq n] \leq \frac{n/2}{n} = 1/2$, so we expect at most 2 tries before $C \leq n$. Letting $t_i = n_i(n_i-1)$ where $n_i$ is the number of collisions in bucket $i$. The amount of space required by this method is given by $\Sigma_{i \in [2^\ell]}t_i = \Sigma_{i \in [2^\ell]} 2 \binom{n_i}{2} = 2 \Sigma_{i \in [2^\ell]} \binom{n_i}{2} = 2C \leq 2n = O(n)$.



\pagebreak

\section*{Summary}
\subsection*{Coupon Collector's Problem}
Given $n$ types of coupons, a trial means picking a coupon independently and at random. Let $X = \text{\# of trials to get all } n \text{ coupon types}$. For $i=0, \hdots, n-1$, the $i$th epoch consists of the trials starting just after the $i$th success and ending in the $(i+1)$th success. Define $X_i = \text{\# of trials in the } i\text{th epoch.}$, then $X=\sum_{i=0}^{n-1} X_i$. Now
\begin{align*}
\mathbf{E}\left[ X \right] = \sum_{i=0}^{n-1} \mathbf{E}\left[ X_i \right] = \sum_{i=0}^{n-1} \frac{n}{n-i} = \sum_{i=1}^{n} \frac{n}{i} =  n \sum_{i=1}^{n} \frac{1}{i} = n H_{n} = n \ln n + O(n)
\end{align*}
Let $\varepsilon_i^r$ denote the event that coupon type $i$ is not picked in the first $r$ trials. Then
\begin{align*}
\mathbf{Pr}\left[ \varepsilon_i^r \right] = \left(1-\frac{1}{n}\right)^r \leq e^{-r/n}
\end{align*}
Pick $r=\beta n \ln n$, where $\beta$ is a constant, then $\mathbf{Pr}\left[ \varepsilon_i^r \right] \leq e^{-\beta \ln n} = n^{-\beta}$ and
\begin{align*}
\mathbf{Pr}\left[ \cup_{i=1}^n \right] \leq \sum_{i=1}^n \mathbf{Pr}\left[ \varepsilon_i^r \right] \leq n^{1-\beta}
\end{align*}
For $\beta=2$, we get $\mathbf{Pr}\left[ \cup_{i=1}^n \right] \leq 1/n$.

\subsection*{Chernoff Bounds}
Given $n$ independent indicator variables $X_1, \hdots, X_n$, $p_i = \mathbf{Pr}\left[ X_i = 1 \right]$, $0 < p_i < 1$ for $i=1, \hdots, n$. Let $X=\sum_{i=1}^n X_i$ and $\mu = \mathbf{E} \left[ X \right] = \sum_{i=1}^n p_i$. Then
\begin{align*}
\mathbf{Pr}\left[ X > (1-\delta)\mu_X \right]
&= \mathbf{Pr}\left[ e^{tX} > e^{t(1-\delta)\mu_X} \right]
< \frac{\mathbf{E}\left[ e^{tX} \right]}{e^{t(1-\delta)\mu_X}}
= \frac{\prod_{i=1}^n \mathbf{E}\left[ e^{tX_i} \right]}{e^{t(1-\delta)\mu_X}} \\
&= \frac{\prod_{i=1}^n \left( p_i e^t + 1-p_i \right)}{e^{t(1-\delta)\mu_X}}
= \frac{\prod_{i=1}^n \left( 1 + p_i (e^t - 1) \right)}{e^{t(1-\delta)\mu_X}} \\
&\leq \frac{\prod_{i=1}^n e^{p_i (e^t-1)}}{e^{t(1-\delta)\mu_X}}
\leq \left( \frac{e^{(e^t-1)}}{e^{t(1-\delta)}} \right)^{\mu_X}
= \left( \frac{e^{\delta}}{\left( 1 + \delta \right)^{1+\delta}} \right)^{\mu_X}
\end{align*}
Furthermore
\begin{align*}
\mathbf{Pr}\left[ X < (1-\delta)\mu_X \right]
&= \mathbf{Pr}\left[ -X < -(1-\delta)\mu_X \right]
= \mathbf{Pr}\left[ e^{-tX} < e^{-t(1-\delta)\mu_X} \right] \\
&< \left( \frac{e^{-\delta}}{\left( 1-\delta \right)^{1-\delta}} \right)^\mu
= \left( \frac{e^{-\delta}}{e^{\left( 1-\delta \right) \ln (1-\delta)}} \right)^\mu \\
&< \left( \frac{e^{-\delta}}{e^{\left( 1-\delta \right)\left( -\delta - \frac{\delta^2}{2} \right)}} \right)^\mu
=  \left( \frac{e^{-\delta}}{e^{-\delta - \frac{\delta^2}{2} + \delta^2 + \frac{\delta^3}{2}}  } \right)^\mu \\
&<  \left( \frac{e^{-\delta}}{e^{-\delta - \frac{\delta^2}{2} + \delta^2}  } \right)^\mu = \left( \frac{1}{e^{-\frac{\delta^2}{2} + \delta^2}  } \right)^\mu
= \left( e^{\frac{\delta^2}{2} - \delta^2} \right)^\mu \\
&= e^{\mu\frac{\delta^2}{2} - \mu\delta^2} = e^{-\mu\sigma^2/2}
\end{align*}

\subsection*{Set-Balancing}
Given a matrix $A \in \left\{ 0,1 \right\}^{n \times n}$, find a vector $\overline{b} \in \left\{ -1, 1 \right\}^n$ s.t. $|| A\overline{b} ||_{\infty}$ is minimized. Pick each entry of $\overline{b}$ to be $1$ with probability $1/2$ and $-1$ otherwise. Now, $\mathbf{E} \left[ || A\overline{b} ||_{\infty} \right] = 0$. Consider a row $i$, $A_{i}$, of $A$, and assume for now that all entries are 1. Let $X$ be the \# of 1's in $\overline{b}$. Now
\begin{align*}
\mathbf{Pr} \left[ | A_i\overline{b} | > 2\sqrt{2n \ln n} \right] &\leq \mathbf{Pr} \left[ X > \frac{n}{2} + \sqrt{2n \ln n} \; \lor \; X < \frac{n}{2} - \sqrt{2n \ln n} \right] \\
&\leq \mathbf{Pr} \left[ X > \frac{n}{2} + \sqrt{2n \ln n} \right] + \mathbf{Pr} \left[ X < \frac{n}{2} - \sqrt{2n \ln n} \right] \\
&\leq \frac{1}{n^2} + \frac{1}{n^2} = \frac{2}{n^2}
\end{align*}
and the third inequality follows from
\begin{align*}
\mathbf{Pr} \left[ X < \frac{n}{2} - \sqrt{2n \ln n} \right]
&= \mathbf{Pr} \left[ X < \left( 1 - \frac{2\sqrt{2n \ln n}}{n} \right)\frac{n}{2} \right]
< e^{-\frac{n}{2} \left( \frac{2\sqrt{2n \ln n}}{n}\right)^2\frac{1}{2}} \\
&< e^{-\frac{n}{4} \left( \frac{8n \ln n}{n^2}\right)} = e^{-2\ln n} = 1/n^2
\end{align*}
where the first inequality follows from applying the second Chernoff bound to the inequality with $\delta=\frac{2\sqrt{2n \ln n}}{n}$ and $\mu=n/2$. Finally applying an union bound over all rows we get
\begin{align*}
\mathbf{Pr} \left[ || A\overline{b} ||_{\infty} > 2\sqrt{2n \ln n} \right] \geq \sum_{i=1}^n \mathbf{Pr} \left[ | A_i\overline{b} | > 2\sqrt{2n \ln n} \right] \leq \sum_{i=1}^n \frac{2}{n^2} = \frac{2}{n}
\end{align*}

\subsection*{Hashing}
Consider $X \subseteq [u]$, $|X| = n$ keys drawn from universe $[u] = [1, \hdots, u-1]$. We wish to store $X$ so as to decide $x \in X$ by using a hash-function $h : [u] \rightarrow [t]$. We say $h$ is universal if given $\forall x,y \in [u]$ and $x \neq y$ then $\mathbf{Pr}\left[ h(x) = h(y) \right] \leq 1/t$. The following are examples of hash functions:
\begin{enumerate}
\item Pick a prime $p \geq u$ and $a \in [p]_+ = [1, \hdots, p-1]$, $b \in [u]$ uniformly at random. Then $h_{a,b}(x) = ((ax + b) \mod p) \mod t$, then $h_{a,b}$ is universal.
\item Let $u=2^w$, $t=2^\ell$ and $\ell \leq w$. Pick a random odd $a \in [2^w]$. Then $h_a(x) = (a * x) \gg (w - \ell)$. Then $\mathbf{Pr}\left[ h_a(x) = h_a(y) \right] \leq 2/2^\ell$.
\end{enumerate}
\paragraph{Naive hashing} We want a worst-case query time of $O(1)$ for deciding the membership problem. Given a hash-function $h$, we construct an array $T[t]$, and set $T[h(x)] = x$ for each $x \in X$. Then $x \in X \Leftrightarrow T[h(x)] = x$. We pick $t$ large enough to expect no collisions. Let $C_{x,y} = h(x) == h(y)$, then the total number of collisions is denoted by $C = \sum_{x \neq y} C_{x,y}$. Now $\mathbf{E}\left[ C \right] = \mathbf{E}\left[ \sum_{x \neq y} C_{x,y} \right] \leq \sum_{x \neq y} 1/t = \binom{n}{2}/t$. By picking $t=n(n-1)=2\binom{n}{2}$, we get $\mathbf{Pr} \left[ C \geq 1 \right] \leq \mathbf{E}\left[ C \right] / 1 \leq 1/2$. If $C \geq 1$, then there is some collision and we try with a new $h$. This approach uses $O(n^2)$ space.

\paragraph{Two-level hashing} Let $n=t$ and $h$ be a hash function. We construct a hashmap as follows:

\begin{algorithm}[H]
 \For{each element $i \in [t]$}{
  store $B_i = \left\{ x \in X \; | \; h(x) = i \right\}$
  }
 \caption{Two-level hashing algorithm}
\end{algorithm}
To answer queries of the type $y \in X$, we can check $y \in B_{h(y)}$. Note that $\mathbf{E} \left[ |B_{h(y)}| \right] = n/t$. Pick $t = n-1$. Let $C$ be defined as before. Then $\mathbf{E} \left[ C \right] = \binom{n}{2}/t = n/2$. If $C > n$, try new $h$. Now $C \leq n$. For $t_i=n_i(n_i-1) = 2\binom{n_i}{2}$, use array $T_i[t_i]$ for each bucket. Now determining whether $y \in X \Leftrightarrow T_i[h_i(y)] = T[h(y)][h_i(y)]$, assuming $h_i$ has no collisions. Otherwise choose a new $h_i$. The space required by this approach is
\begin{align*}
\sum_{i \in [t]} t_i = \sum_{i \in [t]}2\binom{n_i}{2} = 2\sum_{i \in [t]}\binom{n_i}{2} = 2C \leq 2n
\end{align*}
which is linear in $n$.


\end{document}
