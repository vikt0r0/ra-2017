\documentclass[12pt]{article}

% This first part of the file is called the PREAMBLE. It includes
% customizations and command definitions. The preamble is everything
% between \documentclass and \begin{document}.

\usepackage[margin=1in]{geometry}  % set the margins to 1in on all sides
\usepackage{graphicx}              % to include figures
\usepackage{amsmath,bm}            % great math stuff
\usepackage{amsfonts}              % for blackboard bold, etc
\usepackage{amsthm}                % better theorem environments
\usepackage{listings}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage[]{algorithm2e}
\usepackage{parskip} 			   % no paragraph indentation

\usetikzlibrary{arrows,automata}


% various theorems, numbered by section

\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{dfn}[thm]{Definition}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}

\DeclareMathOperator{\id}{id}

\newcommand{\bd}[1]{\mathbf{#1}}  % for bolding symbols
\newcommand{\RR}{\mathbb{R}}      % for Real numbers
\newcommand{\ZZ}{\mathbb{Z}}      % for Integers
\newcommand{\col}[1]{\left[\begin{matrix} #1 \end{matrix} \right]}
\newcommand{\comb}[2]{\binom{#1^2 + #2^2}{#1+#2}}

\lstset{ % General setup for the package
    language={[LaTeX]TeX},
    basicstyle=\footnotesize\sffamily,
    tabsize=4,
    columns=fixed,
    keepspaces,
    commentstyle=\color{red},
    keywordstyle=\color{blue},
    xleftmargin=.1\textwidth,
    xrightmargin=.1\textwidth
}

\begin{document}

\nocite{*}


\title{Randomized Algorithms \\
       Assignment 3}

\author{Viktor Hansen \& Simon Rueskov Schleicher}

\maketitle

\begin{abstract}
  This is the third weekly assignment for the Randomized Algorithms course offered at The Department of Computer Science, Uni. Copenhagen.
\end{abstract}

\pagebreak

\section*{1)}
We wish to determine an upper bound on $\mathbf{Pr} \left[ X > \beta n \ln n \right] \leq \mathbf{Pr} \left[ \left| X - \mu_X \right| \geq \beta n \ln n - \mu_X \right]$. Now, setting $t\sigma_X = \beta n \ln n - \mu_X$, we get $t = \frac{\beta n \ln n - \mu_X}{\sigma_X}$. Inserting and applying Chebyshev's inequality yields
\begin{align*}
\mathbf{Pr} \left[ \left| X - \mu_X \right| \geq \frac{\beta n \ln n - \mu_X}{\sigma_X} \sigma_X \right] \leq \frac{\sigma_X^2}{(\beta n \ln n - \mu_X)^2}
\end{align*}
Inserting $\mu_X = \mathbf{E}Â \left[ X \right] = n \ln n + O(n)$ gives
\begin{align*}
\frac{\sigma_X^2}{(\beta n \ln (n) - \mu_X)^2} &= \frac{\sigma_X^2}{(\beta n \ln (n) - n \ln(n) - O(n))^2} = \frac{\sigma_X^2}{(n \ln(n)(\beta - 1) - O(n))^2} \\
&= \frac{\sigma_X^2}{n^2 \ln^2(n) (\beta - 1)^2 + O(n^2) - O(n^2 \ln(n))} \\
&\leq \frac{\sigma_X^2}{O(n^2 \ln^2(n)) - O(n^2 \ln(n))} \\
&= \frac{\sigma_X^2}{O(n^2 \ln^2(n)} \\
&\leq \lim_{n \rightarrow \infty} \left( O \left( \frac{\sigma_X^2}{n^2} \right) \right) \frac{1}{O(\ln^2(n)} \\
& = O \left( \frac{1}{\ln^2(n)} \right)
\end{align*}
The last two steps follow from $\sigma_X^2/n^2$ being monotonically increasing and converging for large enough $n$.

\section*{2)}

We start by analyzing the root-to-leaf paths in the recursion for QuickSort. We want to show that a path from the root to a leaf is $O(\lg n)$ with high probability. Looking at the path from the root to a specific leaf, we define $X_i$ to be 1 if the node on level $i$ splits the current array into 2 parts, where one of them are at most 3 times bigger than the other. Define $p_i$ to be the probability of $X_i$ being 1, which happens if the pivot was chosen in the middle half (between $\frac{1}{4}$ and $\frac{3}{4}$), so for every $i$ we have $p_i=\frac{1}{2}$. Define $X=\Sigma_{i=1}^k X_i$. So $X$ is the number of good splits in the first $k$ levels. We want to show that for $k=O(\lg n)$ there is a high chance that we have reached a leaf. In the worst case, when we have $X_i=1$ we reduce the array to $\frac{3}{4}$, so when $X\geq lg_{\frac{3}{4}}n$ we know that we have reached a leaf. The probability that we have not reached a leaf in the first $O(\lg n)$ steps is thus (by using Chernoff Bound) $\mathbf{Pr}[X < \log_{\frac{3}{4}}n]=\mathbf{Pr}[X<(1-\delta)O(\lg n)] < exp(-O(\lg n)\delta^2/2)=O(m^{-1})$. That is, for large $n$ there is a high probability that we will have reached a leaf in $O(\lg n)$ steps. So we can conclude that RandQS runs in time $O(n \lg n)$.


\section*{3)}
Consider a treap $T=(e_1, e_2, ..., e_n) = ((v_1,p_1), (v_2,p_2), ..., (v_n,p_n))$ with an element $e_k = (v_k, -\infty) \in T$ of rank $k$. We wish to analyze the expected depth of $e_j$ in $T$. Let $X_i = 1$ if $e_i$ is an ancestor of $e_k$ and $X_i=0$ otherwise. Then the depth of $e_k$ is given by $X=1 + \sum_{i=1}^{n}X_i$ and
\begin{align*}
\mathbf{E}\left[ X \right] = 1 + \mathbf{E}\left[ \sum\limits_{i=1}^{n}X_i \right] = 1 + \sum\limits_{i=1}^{n}\mathbf{E}\left[X_i \right] 
\end{align*}
Since $e_k$ has rank $k$, $e_i$ is an ancestor of $e_k$ exactly when $e_i$ has the highest priority of the elements $e_i, ..., e_k$, when $i < k$ and $e_k, ..., e_i$ otherwise. Since the priorities are chosen at random, the probabilities of these events are given by $1/(k-i)$ and $1/(i-k)$, respectively. We then have $\mathbf{Pr}\left[ X_i = 1 \right] = 1 / |i-k|$ and
\begin{align*}
1 + \sum\limits_{i=1}^{n}\mathbf{E}\left[X_i \right] &= 1 + \sum\limits_{i=1}^{n}\mathbf{Pr}\left[X_i = 1 \right] \\
&= 1 + \sum\limits_{i=1}^{n} 1 / |i-k| \\
&= 1 +\sum\limits_{i=1}^{k-1} \frac{1}{k-i} + \sum\limits_{i=k+1}^{n} \frac{1}{i-k} \\
&= 1 +H_{k-1} + H_{n-k}
\end{align*}
Subtracting the expected depth of $e_k$ from the expected depth of an element with rank $k$ and a random priority (lemma 8.6) yields
\begin{align*}
1 + H_{k-1} + H_{n-k} - H_k - H_{n-k+1} + 1 &= 1 + (H_{k-1} - H_k) + (H_{n-k} - H_{n-k+1}) + 1 \\
&= 1 -\frac{1}{k} +1 - \frac{1}{n-k+1} \\
&= \mathbf{E}\left[ R_x \right] + \mathbf{E}\left[ L_x \right]
\end{align*}
which exactly is the expected number of rotations required when deleting an element of rank $k$. This makes sense, because when deleting the element, we change its priority to $\infty$ and make rotations until it reaches the bottom. Each rotation moves it down 1 level. So the depth of the bottom (when the priority is negative infinity) should be the depth when we started plus the number of rotations.


\section*{4)}
Consider the 2-level hashing scheme with $h_a$ used for the hash-function. Pick random, odd $a \in [2^w]$, and let $u = 2^w$ and $t = 2^\ell$ where $\ell \leq w$.

\begin{algorithm}[H]
 \For{each element $i \in [t]$}{
  store $B_i = \left\{ x \in X \; | \; h(x) = i \right\}$
  }
 \caption{Two-level hashing algorithm}
\end{algorithm}

Each bucket $B_i$ is internally represented by a secondary hash-table, $T_i[t_i]$. Let $C_{x,y} = h_a(x) == h_a(y)$, i.e. if there is a collision between $x$ and $y$ and $C = \sum_{x \neq y} C_{x,y}$ denote t he total number of collisions. Then $\mathbf{E} \left[ C \right] = \mathbf{E}\left[ \sum_{x \neq y} C_{x,y} \right] \leq \sum_{x \neq y} 2/2^{\ell} = \binom{n}{2}2/2^\ell = n(n-1)/2^\ell$. For a set of size $n$, we choose $\ell = \lfloor \log_2(n-1) \rfloor + 2$, for which $\mathbf{E}[C] = n(n-1)/2^{\lfloor \log_2(n-1) \rfloor + 2} \leq n(n-1)/2^{\log_2(n-1) + 1} = n/2$. Pick $h_a$ s.t. $C \leq n$. Then from applying Markov's inequality we get that $\mathbf{Pr}[X \geq n] \leq \frac{n/2}{n} = 1/2$, so we expect at most 2 tries before $C \leq n$. Letting $t_i = 2^{\lfloor \log_2 n_i(n_i-1) \rfloor} \leq 2^{\log_2 n_i(n_i-1)+1} = 2n_i(n_i-1) = 4 \binom{n_i}{2}$ where $n_i$ is the number of collisions in bucket $i$. Another hash function $h_{a_i}$ is chosen so that it perfectly hashes the elements of bucket $i$ into the hash-table $T_i$. The amount of space required by this method is given by $\Sigma_{i \in [2^\ell]}t_i = \Sigma_{i \in [2^\ell]} 4 \binom{n_i}{2} = 4 \Sigma_{i \in [2^\ell]} \binom{n_i}{2} = 4C \leq 4n = O(n)$.

\pagebreak

\section*{Summary}
\subsection*{Coupon Collector's Problem}
Given $n$ types of coupons, a trial means picking a coupon independently and at random. Let $X = \text{\# of trials to get all } n \text{ coupon types}$. For $i=0, \hdots, n-1$, the $i$th epoch consists of the trials starting just after the $i$th success and ending in the $(i+1)$th success. Define $X_i = \text{\# of trials in the } i\text{th epoch.}$, then $X=\sum_{i=0}^{n-1} X_i$. Now
\begin{align*}
\mathbf{E}\left[ X \right] = \sum_{i=0}^{n-1} \mathbf{E}\left[ X_i \right] = \sum_{i=0}^{n-1} \frac{n}{n-i} = \sum_{i=1}^{n} \frac{n}{i} =  n \sum_{i=1}^{n} \frac{1}{i} = n H_{n} = n \ln n + O(n)
\end{align*}
Let $\varepsilon_i^r$ denote the event that coupon type $i$ is not picked in the first $r$ trials. Then
\begin{align*}
\mathbf{Pr}\left[ \varepsilon_i^r \right] = \left(1-\frac{1}{n}\right)^r \leq e^{-r/n}
\end{align*}
Pick $r=\beta n \ln n$, where $\beta$ is a constant, then $\mathbf{Pr}\left[ \varepsilon_i^r \right] \leq e^{-\beta \ln n} = n^{-\beta}$ and
\begin{align*}
\mathbf{Pr}\left[ \cup_{i=1}^n \right] \leq \sum_{i=1}^n \mathbf{Pr}\left[ \varepsilon_i^r \right] \leq n^{1-\beta}
\end{align*}
For $\beta=2$, we get $\mathbf{Pr}\left[ \cup_{i=1}^n \right] \leq 1/n$.

\subsection*{Chernoff Bounds}
Given $n$ independent indicator variables $X_1, \hdots, X_n$, $p_i = \mathbf{Pr}\left[ X_i = 1 \right]$, $0 < p_i < 1$ for $i=1, \hdots, n$. Let $X=\sum_{i=1}^n X_i$ and $\mu = \mathbf{E} \left[ X \right] = \sum_{i=1}^n p_i$. Then
\begin{align*}
\mathbf{Pr}\left[ X > (1-\delta)\mu_X \right]
&= \mathbf{Pr}\left[ e^{tX} > e^{t(1-\delta)\mu_X} \right]
< \frac{\mathbf{E}\left[ e^{tX} \right]}{e^{t(1-\delta)\mu_X}}
= \frac{\prod_{i=1}^n \mathbf{E}\left[ e^{tX_i} \right]}{e^{t(1-\delta)\mu_X}} \\
&= \frac{\prod_{i=1}^n \left( p_i e^t + 1-p_i \right)}{e^{t(1-\delta)\mu_X}}
= \frac{\prod_{i=1}^n \left( 1 + p_i (e^t - 1) \right)}{e^{t(1-\delta)\mu_X}} \\
&\leq \frac{\prod_{i=1}^n e^{p_i (e^t-1)}}{e^{t(1-\delta)\mu_X}}
\leq \left( \frac{e^{(e^t-1)}}{e^{t(1-\delta)}} \right)^{\mu_X}
= \left( \frac{e^{\delta}}{\left( 1 + \delta \right)^{1+\delta}} \right)^{\mu_X}
\end{align*}
Furthermore
\begin{align*}
\mathbf{Pr}\left[ X < (1-\delta)\mu_X \right]
&= \mathbf{Pr}\left[ -X < -(1-\delta)\mu_X \right]
= \mathbf{Pr}\left[ e^{-tX} < e^{-t(1-\delta)\mu_X} \right] \\
&< \left( \frac{e^{-\delta}}{\left( 1-\delta \right)^{1-\delta}} \right)^\mu
= \left( \frac{e^{-\delta}}{e^{\left( 1-\delta \right) \ln (1-\delta)}} \right)^\mu \\
&< \left( \frac{e^{-\delta}}{e^{\left( 1-\delta \right)\left( -\delta - \frac{\delta^2}{2} \right)}} \right)^\mu
=  \left( \frac{e^{-\delta}}{e^{-\delta - \frac{\delta^2}{2} + \delta^2 + \frac{\delta^3}{2}}  } \right)^\mu \\
&<  \left( \frac{e^{-\delta}}{e^{-\delta - \frac{\delta^2}{2} + \delta^2}  } \right)^\mu = \left( \frac{1}{e^{-\frac{\delta^2}{2} + \delta^2}  } \right)^\mu
= \left( e^{\frac{\delta^2}{2} - \delta^2} \right)^\mu \\
&= e^{\mu\frac{\delta^2}{2} - \mu\delta^2} = e^{-\mu\sigma^2/2}
\end{align*}

\subsection*{Set-Balancing}
Given a matrix $A \in \left\{ 0,1 \right\}^{n \times n}$, find a vector $\overline{b} \in \left\{ -1, 1 \right\}^n$ s.t. $|| A\overline{b} ||_{\infty}$ is minimized. Pick each entry of $\overline{b}$ to be $1$ with probability $1/2$ and $-1$ otherwise. Now, $\mathbf{E} \left[ || A\overline{b} ||_{\infty} \right] = 0$. Consider a row $i$, $A_{i}$, of $A$, and assume for now that all entries are 1. Let $X$ be the \# of 1's in $\overline{b}$. Now
\begin{align*}
\mathbf{Pr} \left[ | A_i\overline{b} | > 2\sqrt{2n \ln n} \right] &\leq \mathbf{Pr} \left[ X > \frac{n}{2} + \sqrt{2n \ln n} \; \lor \; X < \frac{n}{2} - \sqrt{2n \ln n}Â \right] \\
&\leq \mathbf{Pr} \left[ X > \frac{n}{2} + \sqrt{2n \ln n} \right] + \mathbf{Pr} \left[ X < \frac{n}{2} - \sqrt{2n \ln n}Â \right] \\
&\leq \frac{1}{n^2} + \frac{1}{n^2} = \frac{2}{n^2}
\end{align*}
and the third inequality follows from
\begin{align*}
\mathbf{Pr} \left[ X < \frac{n}{2} - \sqrt{2n \ln n} \right]
&= \mathbf{Pr} \left[ X < \left( 1 - \frac{2\sqrt{2n \ln n}}{n} \right)\frac{n}{2} \right]
< e^{-\frac{n}{2} \left( \frac{2\sqrt{2n \ln n}}{n}\right)^2\frac{1}{2}} \\
&< e^{-\frac{n}{4} \left( \frac{8n \ln n}{n^2}\right)} = e^{-2\ln n} = 1/n^2
\end{align*}
where the first inequality follows from applying the second Chernoff bound to the inequality with $\delta=\frac{2\sqrt{2n \ln n}}{n}$ and $\mu=n/2$. Finally applying an union bound over all rows we get
\begin{align*}
\mathbf{Pr} \left[ || A\overline{b} ||_{\infty} > 2\sqrt{2n \ln n} \right] \geq \sum_{i=1}^n \mathbf{Pr} \left[ | A_i\overline{b} | > 2\sqrt{2n \ln n} \right] \leq \sum_{i=1}^n \frac{2}{n^2} = \frac{2}{n}
\end{align*}

\subsection*{Hashing}
Consider $X \subseteq [u]$, $|X| = n$ keys drawn from universe $[u] = [1, \hdots, u-1]$. We wish to store $X$ so as to decide $x \in X$ by using a hash-function $h : [u] \rightarrow [t]$. We say $h$ is universal if given $\forall x,y \in [u]$ and $x \neq y$ then $\mathbf{Pr}\left[ h(x) = h(y) \right] \leq 1/t$. The following are examples of hash functions:
\begin{enumerate}
\item Pick a prime $p \geq u$ and $a \in [p]_+ = [1, \hdots, p-1]$, $b \in [u]$ uniformly at random. Then $h_{a,b}(x) = ((ax + b) \mod p) \mod t$, then $h_{a,b}$ is universal.
\item Let $u=2^w$, $t=2^\ell$ and $\ell \leq w$. Pick a random odd $a \in [2^w]$. Then $h_a(x) = (a * x) \gg (w - \ell)$. Then $\mathbf{Pr}\left[ h_a(x) = h_a(y) \right] \leq 2/2^\ell$.
\end{enumerate}
\paragraph{Naive hashing} We want a worst-case query time of $O(1)$ for deciding the membership problem. Given a hash-function $h$, we construct an array $T[t]$, and set $T[h(x)] = x$ for each $x \in X$. Then $x \in X \Leftrightarrow T[h(x)] = x$. We pick $t$ large enough to expect no collisions. Let $C_{x,y} = h(x) == h(y)$, then the total number of collisions is denoted by $C = \sum_{x \neq y} C_{x,y}$. Now $\mathbf{E}\left[ C \right] = \mathbf{E}\left[ \sum_{x \neq y} C_{x,y} \right] \leq \sum_{x \neq y} 1/t = \binom{n}{2}/t$. By picking $t=n(n-1)=2\binom{n}{2}$, we get $\mathbf{Pr} \left[ C \geq 1 \right] \leq \mathbf{E}\left[ C \right] / 1 \leq 1/2$. If $C \geq 1$, then there is some collision and we try with a new $h$. This approach uses $O(n^2)$ space.

\paragraph{Two-level hashing} Let $n=t$ and $h$ be a hash function. We construct a hashmap as follows:

\begin{algorithm}[H]
 \For{each element $i \in [t]$}{
  store $B_i = \left\{ x \in X \; | \; h(x) = i \right\}$
  }
 \caption{Two-level hashing algorithm}
\end{algorithm}
To answer queries of the type $y \in X$, we can check $y \in B_{h(y)}$. Note that $\mathbf{E} \left[ |B_{h(y)}| \right] = n/t$. Pick $t = n-1$. Let $C$ be defined as before. Then $\mathbf{E} \left[ C \right] = \binom{n}{2}/t = n/2$. If $C > n$, try new $h$. Now $C \leq n$. For $t_i=n_i(n_i-1) = 2\binom{n_i}{2}$, use array $T_i[t_i]$ for each bucket. Now determining whether $y \in X \Leftrightarrow T_i[h_i(y)] = T[h(y)][h_i(y)]$, assuming $h_i$ has no collisions. Otherwise choose a new $h_i$. The space required by this approach is
\begin{align*}
\sum_{i \in [t]} t_i = \sum_{i \in [t]}2\binom{n_i}{2} = 2\sum_{i \in [t]}\binom{n_i}{2} = 2C \leq 2n
\end{align*}
which is linear in $n$.

\subsection*{Random Treaps}
A treap is a special form of a binary search tree that is also a heap.\\
When we need to store items with totally ordered keys, and later search for them, a binary search tree is a good option for a data structure. But there's many different ways to build a binary tree with a set of items. The more balanced a binary tree is, the better, because we need less time to search for item. Treaps are a randomized way of building a balanced binary tree.\\
When building a treap, each item gets a random number as their priority, and then a tree is build, that is a binary search tree with respect to the keys and a heap with respect to the priorities.\\
Proof that such a tree exists: When given a set of items with keys $k_i$ and priorities $p_i$, we can construct such treap: First we take the item with the highest priority. We call that item $x$. $x$ has to be at the root of the tree, because it has the highest priority. All items with key less than $k_x$ has to be in the left subtree, and all items with keys more than $k_x$ has to be in the right subtree. Now we can recursively build both the left subtree and the right subtree (the items to the left and right with the highest priorities has to be at the root of the subtrees, and we can divide the remaining items according to the keys). This proves that from any given set of items with keys and priorities, we can construct a treap containing those items, and furthermore it proves that it can be done in a unique way.\\
With random priorities, treaps work very much like the Quicksort algorithm. Byt choosing random priorities to each of the items, it becomes random which of the items gets the highest priority. That item has to be in the root of the tree, and all other items has to be in either the left or right subtree according to whether their keys are less or more than the key of the item with the highest priority. This works in the same way as choosing a random item and partitioning according to it in the Quicksort algorithm. After the partitioning, we do the same recursively for both subtrees in the treap individually, like we do the same recursively for both partitions in the Quicksort algorithm. So treaps and the quicksort algorithm work in the same way. Since we know that the quicksort algorithm has expected $O(logn)$ layers, we can conclude that a random treap gets and expected height of $O(logn)$.\\
\textbf{Lemma 8.6:} Let T be a random treap for a set S of size n. For an element $x\in S$ having rank k, $$E[depth(x)]=H_k+H_{n-k+1}-1$$
$L_x$ = length of the left spine of the right sub-tree of x.\\
$R_x$ = lenght of the right spine of the left sub-tree of x.\\
\textbf{Lemma 8.7:} Let T be a random treap for a set S of size n. For an element $x\in S$ of rank k, $$E[R_x]=1-\frac{1}{k}$$ and $$E[L_x]=1-\frac{1}{n-k+1}.$$
\textbf{Theorem 8.8:} Let T be a random treap for a set S of size n. \begin{enumerate}\item The expected time for a FIND, INSERT, or DELETE operation on T is $O(logn)$.\item The expected number of rotations required during an INSERT or DELETE operation is at most 2.\item The expected time for a JOIN, PASTE, or SPLIT operation involving sets $S_1$ and $S_2$ of sizes n and m, respectively, is $O(logn+logm)$.
\end{enumerate}

\end{document}
